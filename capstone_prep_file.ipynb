{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Restaurant revenue forecast**\n",
    "\n",
    "In this notebook, we have code that builds a deep learning regression model that predicts monthly revenue for a restaurant. \n",
    "\n",
    "#### Notebook structure (**follows CRISP-DM framework**)\n",
    "\n",
    "1. Business value<br>\n",
    "2. Exploatory Data Analysis<br>\n",
    "3. Data Selection<br>\n",
    "4. Feature selection<br>\n",
    "5. Modellingt<br>\n",
    "6. Evaluation<br>\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\">Part I : Business understanding</label>\n",
    "Forecasting business mertics is very important for all businesses. I helps businesses be proactive. \n",
    "Imagine you know next month your business : <br/><br/>\n",
    "`1`. is not going to make profit,meaning you are going to make a loss, <br/><br/>\n",
    "`2`. with that knowledge you can react proactively by putting campaigns that are going to help you avoid running a loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    explained_variance_score, median_absolute_error\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from csv to dataframe\n",
    "restaurant_data = pd.read_csv(\"Restaurant_revenue.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\" id=\"Exploratory-Data-Analysis\">Part II :Data understanding (Exploratory Data Analysis)</label>\n",
    "\n",
    "\n",
    "`1.` What is the distribution of all numeric variables?  <br/>\n",
    "`2.` What is the distribution of our target variable, monthly revenue? <br/>\n",
    "`3.` What is the distribution of customer spending, menu price, and number of customer? <br/>\n",
    "`4.` What is the statistical attributes of the data looking at numerical columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data dimensions\n",
    "restaurant_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check columns and their datatypes\n",
    "restaurant_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show data statistical attributes\n",
    "restaurant_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show cuisine type distributions\n",
    "restaurant_data.Cuisine_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Promotions distributions\n",
    "restaurant_data.Promotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variable relationships and distributions having hue as cuisine type\n",
    "restaurant_data = restaurant_data.loc[:, ~restaurant_data.columns.duplicated()]\n",
    "required_columns = ['Number_of_Customers', 'Menu_Price', 'Marketing_Spend', 'Average_Customer_Spending', 'Promotions', 'Reviews','Monthly_Revenue','Cuisine_Type']\n",
    "missing_columns = [col for col in required_columns if col not in restaurant_data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "training_data_cleaned = restaurant_data.dropna(subset=required_columns)\n",
    "training_data_cleaned['Cuisine_Type'] = training_data_cleaned['Cuisine_Type'].astype('category')\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.set_style('whitegrid')\n",
    "sns.pairplot(training_data_cleaned[required_columns], height=2, hue='Cuisine_Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variable relationships and distributions having hue as promotion\n",
    "restaurant_data = restaurant_data.loc[:, ~restaurant_data.columns.duplicated()]\n",
    "required_columns = ['Number_of_Customers', 'Menu_Price', 'Marketing_Spend', 'Average_Customer_Spending', 'Promotions', 'Reviews','Monthly_Revenue','Cuisine_Type']\n",
    "missing_columns = [col for col in required_columns if col not in restaurant_data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "training_data_cleaned = restaurant_data.dropna(subset=required_columns)\n",
    "training_data_cleaned['Promotions'] = training_data_cleaned['Promotions'].astype('category')\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.set_style('whitegrid')\n",
    "sns.pairplot(training_data_cleaned[required_columns], height=2, hue='Promotions',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variable means having hue as promotion\n",
    "plt.figure(figsize=(10, 3))\n",
    "df_melted = restaurant_data.melt(id_vars='Promotions', value_vars=['Number_of_Customers', 'Menu_Price', 'Marketing_Spend', 'Average_Customer_Spending', 'Reviews','Monthly_Revenue'], var_name='variable', value_name='value')\n",
    "mean_values = df_melted.groupby(['variable', 'Promotions']).mean().reset_index()\n",
    "sns.barplot(data=mean_values, x='variable', y='value', hue='Promotions')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Averages')\n",
    "plt.title('Promotions distribution across all variable')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variable means having hue as cuisine type\n",
    "plt.figure(figsize=(10, 3))\n",
    "df_melted = restaurant_data.melt(id_vars='Cuisine_Type', value_vars=['Number_of_Customers', 'Menu_Price', 'Marketing_Spend', 'Average_Customer_Spending', 'Reviews','Monthly_Revenue'], var_name='variable', value_name='value')\n",
    "mean_values = df_melted.groupby(['variable', 'Cuisine_Type']).mean().reset_index()\n",
    "sns.barplot(data=mean_values, x='variable', y='value', hue='Cuisine_Type')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Averages')\n",
    "plt.title('Cuisines distribution across all variable')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show variables distribution\n",
    "\n",
    "\n",
    "# Creating the histogram with Seaborn\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(list(restaurant_data['Monthly_Revenue']), bins=8, kde=True, color='red', edgecolor='blue')\n",
    "plt.title('Monthly revenue distribution', fontsize=16)\n",
    "plt.xlabel('Revenue', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Creating the histogram with Seaborn\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(list(restaurant_data['Average_Customer_Spending']), bins=8, kde=True, color='red', edgecolor='blue')\n",
    "plt.title('Average customer spending distribution', fontsize=16)\n",
    "plt.xlabel('Spending', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Creating the histogram with Seaborn\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(list(restaurant_data['Marketing_Spend']), bins=8, kde=True, color='red', edgecolor='blue')\n",
    "plt.title('Marketing spend distribution', fontsize=16)\n",
    "plt.xlabel('Spend', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Creating the histogram with Seaborn\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(list(restaurant_data['Number_of_Customers']), bins=8, kde=True, color='red', edgecolor='blue')\n",
    "plt.title('Number of customers distribution', fontsize=16)\n",
    "plt.xlabel('Customers', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Creating the histogram with Seaborn\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(list(restaurant_data['Menu_Price']), bins=8, kde=True, color='red', edgecolor='blue')\n",
    "plt.title('Menu price distribution', fontsize=16)\n",
    "plt.xlabel('Menu price', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\" id=\"Exploratory-Data-Analysis\">Part III : Data preparation</label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_percentages(df):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of missing values in each column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas DataFrame): The DataFrame for which missing values percentages are calculated.\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame: A DataFrame containing two columns:\n",
    "        - 'column_name': The name of each column in the input DataFrame.\n",
    "        - 'percent_missing': The percentage of missing values in each column.\n",
    "    \"\"\"\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                     'percent_missing': percent_missing})\n",
    "    return missing_value_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ceating a dataframe that column and missing values percentage\n",
    "cust_data_missing = missing_value_percentages(restaurant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_data_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness_detector(dataset, col):\n",
    "    \"\"\"\n",
    "    Detect the skewness of a specified column in a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pandas DataFrame): The dataset containing the column.\n",
    "    - col (str): The name of the column for which skewness is to be detected.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string indicating the skewness of the column's distribution:\n",
    "        - \"left-skewed\" if the distribution is left-skewed.\n",
    "        - \"right-skewed\" if the distribution is right-skewed.\n",
    "        - \"symmetrical\" if the distribution is symmetrical.\n",
    "    \"\"\"\n",
    "    skewness = skew(dataset[col])\n",
    "    if skewness > 0:\n",
    "        return \"right-skewed\"\n",
    "    elif skewness < 0:\n",
    "        return \"left-skewed\"\n",
    "    else:\n",
    "        return \"symmetrical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting numerical columns from the dataframe\n",
    "numeric_columns = restaurant_data.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining columns skewness\n",
    "cols_skewness = pd.DataFrame()\n",
    "for column in numeric_columns:\n",
    "    skewness = skewness_detector(restaurant_data,column)\n",
    "    row = {'column':column,'skewness':skewness}\n",
    "    cols_skewness = cols_skewness.append(row, ignore_index=True)\n",
    "cols_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(data_frame, cols):\n",
    "    \"\"\"\n",
    "    Remove outliers from the specified columns of a DataFrame using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Parameters:\n",
    "    - data_frame (pandas DataFrame): The DataFrame containing the data.\n",
    "    - cols (list of str): A list of column names from which outliers should be removed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame: A DataFrame with outliers removed from the specified columns.\n",
    "    \"\"\"\n",
    "    cleaned_df = data_frame.copy()\n",
    "    for column_name in cols:\n",
    "        Q1 = cleaned_df[column_name].quantile(0.25)\n",
    "        Q3 = cleaned_df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        cleaned_df = cleaned_df[(cleaned_df[column_name] >= lower_bound) & (cleaned_df[column_name] <= upper_bound)]\n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe where outliers are removed\n",
    "restaurant_data_wou = remove_outliers_iqr(restaurant_data,numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing statistical attributes of our data when outlier are removed\n",
    "restaurant_data_wou.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates\n",
    "has_duplicates = restaurant_data_wou.duplicated().any()\n",
    "\n",
    "if has_duplicates:\n",
    "    print(\"DataFrame has duplicates.\")\n",
    "else:\n",
    "    print(\"DataFrame has no duplicates.\")\n",
    "\n",
    "# Get duplicate counts\n",
    "duplicate_counts = restaurant_data_wou.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicate rows:\", duplicate_counts)\n",
    "restaurant_data_wou = restaurant_data_wou.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying onehot encoding on nominal variable\n",
    "restaurant_data_wou = pd.get_dummies(restaurant_data_wou, columns=['Cuisine_Type']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\" id=\"Exploratory-Data-Analysis\">Part IV : Feature selection</label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assigning target and predictor variables\n",
    "X = restaurant_data_wou.drop(columns=['Monthly_Revenue'])\n",
    "y = restaurant_data_wou['Monthly_Revenue']\n",
    "feature_names = X.columns\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on variance thresholding.\n",
    "\n",
    "    Parameters:\n",
    "    - data (array-like or sparse matrix): The input data.\n",
    "    - threshold (float, optional): The threshold below which features will be removed. \n",
    "                                   Features with a variance lower than this threshold will be removed.\n",
    "                                   Default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two elements:\n",
    "        - Transformed data after removing features with low variance.\n",
    "        - Indices of the selected features.\n",
    "    \"\"\"\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    return selector.fit_transform(data), selector.get_support(indices=True)\n",
    "\n",
    "\n",
    "# Apply Variance Threshold to the training and testing sets\n",
    "X_train_var, selected_indices_var = variance_threshold_selector(X_train, threshold=0.01)\n",
    "X_test_var = X_test.iloc[:, selected_indices_var].values  # Apply same indices to test set\n",
    "\n",
    "# Update feature names after Variance Threshold\n",
    "feature_names_var = feature_names[selected_indices_var]\n",
    "\n",
    "def correlation_coefficient_selector(X, y, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Select features based on their correlation coefficient with the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): The feature matrix.\n",
    "    - y (array-like): The target variable.\n",
    "    - threshold (float, optional): The threshold for selecting features based on correlation coefficient.\n",
    "                                   Default is 0.2.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list containing the indices of the selected features.\n",
    "    \"\"\"\n",
    "    selected_features = []\n",
    "    for i in range(X.shape[1]):\n",
    "        corr, _ = pearsonr(X[:, i], y)\n",
    "        if abs(corr) >= threshold:\n",
    "            selected_features.append(i)\n",
    "    return selected_features\n",
    "\n",
    "# Get the selected feature indices based on correlation coefficient\n",
    "selected_features_corr = correlation_coefficient_selector(X_train_var, y_train, threshold=0.2)\n",
    "\n",
    "# Reduce data to selected features\n",
    "X_train_selected = X_train_var[:, selected_features_corr]\n",
    "X_test_selected = X_test_var[:, selected_features_corr]\n",
    "\n",
    "# Update feature names after Correlation Coefficient\n",
    "feature_names_selected = feature_names_var[selected_features_corr]\n",
    "print(f'Selected Feature Names: {list(feature_names_selected)}')\n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = StandardScaler()\n",
    "X_train_selected_normalized = X_train_selected\n",
    "X_test_selected_normalized = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\" id=\"Exploratory-Data-Analysis\">Part V : Modelling</label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP Regressor with selected features\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_selected_normalized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\" id=\"Exploratory-Data-Analysis\">Part VI : Evaluation</label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = mlp.predict(X_test_selected_normalized)\n",
    "y_pred_train = mlp.predict(X_train_selected_normalized)\n",
    "\n",
    "# Calculate evaluation metrics (Test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1-r2) * (len(y_test)-1) / (len(y_test)-X_test_selected_normalized.shape[1]-1)\n",
    "explained_variance = explained_variance_score(y_test, y_pred)\n",
    "median_ae = median_absolute_error(y_test, y_pred)\n",
    "# Calculate evaluation metrics( Train)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mape_train = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "adjusted_r2_train = 1 - (1-r2_train) * (len(y_pred_train)-1) / (len(y_pred_train)-X_test_selected_normalized.shape[1]-1)\n",
    "explained_variance_train = explained_variance_score(y_train, y_pred_train)\n",
    "median_ae_train = median_absolute_error(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign evaluation metrics for test set\n",
    "metrics_test = {\n",
    "    'Mean Squared Error (MSE)': mse,\n",
    "    'Root Mean Squared Error (RMSE)': rmse,\n",
    "    'Mean Absolute Error (MAE)': mae,\n",
    "    'Mean Absolute Percentage Error (MAPE)': mape,\n",
    "    'R-squared (R2)': r2,\n",
    "    'Adjusted R-squared (Adj R2)': adjusted_r2,\n",
    "    'Explained Variance Score': explained_variance,\n",
    "    'Median Absolute Error (MedAE)': median_ae\n",
    "}\n",
    "\n",
    "# assign evaluation metrics for training set\n",
    "metrics_train = {\n",
    "    'Mean Squared Error (MSE)': mse_train,\n",
    "    'Root Mean Squared Error (RMSE)': rmse_train,\n",
    "    'Mean Absolute Error (MAE)': mae_train,\n",
    "    'Mean Absolute Percentage Error (MAPE)': mape_train,\n",
    "    'R-squared (R2)': r2_train,\n",
    "    'Adjusted R-squared (Adj R2)': adjusted_r2_train,\n",
    "    'Explained Variance Score': explained_variance_train,\n",
    "    'Median Absolute Error (MedAE)': median_ae_train\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "df_test = pd.DataFrame(metrics_test, index=[''])\n",
    "df_train = pd.DataFrame(metrics_train, index=[''])\n",
    "\n",
    "\n",
    "# Create a DataFrame with both test and training values and their percentage difference\n",
    "df_combined = pd.concat([df_test, df_train], keys=['Test', 'Train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for MLPRegressor\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_selected_normalized, y_train)\n",
    "\n",
    "best_mlp = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_mlp.predict(X_test_selected_normalized)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1-r2) * (len(y_test)-1) / (len(y_test)-X_test_selected_normalized.shape[1]-1)\n",
    "explained_variance = explained_variance_score(y_test, y_pred)\n",
    "median_ae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'Selected Feature Names: {list(feature_names_selected)}')\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Absolute Percentage Error: {mape}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Adjusted R-squared: {adjusted_r2}')\n",
    "print(f'Explained Variance Score: {explained_variance}')\n",
    "print(f'Median Absolute Error: {median_ae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <label style=\"color:blue\" id=\"Exploratory-Data-Analysis\">Part VII : Deployment</label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"model_artifact.pkl\", 'wb') as file:\n",
    "        pickle.dump(mlp, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
